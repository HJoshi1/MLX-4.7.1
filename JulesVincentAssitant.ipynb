{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066f4863-dec1-41f4-84a8-2da79de6c86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af754bb949a94754a909f4dc422193be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mixtral-8x7B-Instruct-v0.1\", load_in_4bit=True, torch_dtype=torch.float16, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "024f5d5a-7f02-4f1e-b94f-b440e503ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "tokenizer.pad_token = \"!\"\n",
    "CUTOFF_LEN = 128\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 2 * LORA_R\n",
    "LORA_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6842b12-9a0d-4130-80e5-badba3b7a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, target_modules=[ \"w1\", \"w2\", \"w3\"], lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f96251a2-eeaa-420f-b520-672db39e806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"harpreetsahota/modern-to-shakesperean-translation\")\n",
    "# print(\"dataset\", dataset)\n",
    "# train_data = dataset[\"train\"]\n",
    "\n",
    "# # Print the first few examples from the training dataset\n",
    "# print(\"First few examples from the training dataset:\")\n",
    "# for i in range(3):  # Adjust the range to preview more or fewer examples\n",
    "#     print(f\"Example {i+1}: {train_data[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a80c0402-698e-4298-8d0b-b6d6a9d5e5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error rows: 0\n",
      "690\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Function to load JSONs from a CSV file\n",
    "def load_jsons_from_csv(filename):\n",
    "    data = []  # This list will store all the JSON objects\n",
    "    errors = []\n",
    "    with open(filename, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            if row:  # Ensure the row is not empty\n",
    "                # Parse JSON from the first column in each row (assuming each row is a JSON string)\n",
    "                try:\n",
    "                    json_str = ','.join(part for part in row if part.strip())\n",
    "                    # print(\"Json string\")\n",
    "                    # print(json_str)\n",
    "                    # print(type(json_str))\n",
    "                    json_data = json.loads(json_str)\n",
    "                    # print(\"Json data\")\n",
    "                    # print(json_data)\n",
    "                    data_list = [json_data]  # Place the dictionary in a list as per your requirement\n",
    "                    # print(\"Data list\")\n",
    "                    # print(data_list)\n",
    "                    data.append(json_data)\n",
    "                except json.JSONDecodeError:\n",
    "                    errors.append(row)\n",
    "                    pass\n",
    "    print(\"Number of error rows: \" + str(len(errors))) \n",
    "    return data, errors\n",
    "\n",
    "# Example usage\n",
    "filename = 'Jules_Dialogues_JSON.csv'\n",
    "loaded_jsons, errors = load_jsons_from_csv(filename)\n",
    "# print(loaded_jsons)\n",
    "# for error in errors:\n",
    "#     print(error)\n",
    "\n",
    "# print()\n",
    "# print()\n",
    "\n",
    "# for data in loaded_jsons:\n",
    "#     print(data)\n",
    "\n",
    "print(len(loaded_jsons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92ea6c3b-9754-4a05-8b30-eab9364c24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690\n"
     ]
    }
   ],
   "source": [
    "# {'role': 'user', 'content': '(yelling to all) Everybody be cool this is a robbery!'},\n",
    " # {'role': 'assistant', 'content': ' Okay now, tell me about the hash bars?'},\n",
    "\n",
    "def generate_prompt(usercontent, assistant_content):\n",
    "  sys_msg = \"Respond to this in the form of Jules from Pulp Fiction\"\n",
    "  p = \"<s> [INST]\" + sys_msg +\"\\n\"+ usercontent[\"content\"] + \"[/INST]\" +  assistant_content[\"content\"] + \"</s>\"\n",
    "  return p \n",
    "\n",
    "\n",
    "tokenize = lambda prompt: tokenizer(prompt + tokenizer.eos_token, truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\")\n",
    "\n",
    "train_data = []\n",
    "\n",
    "print(len(loaded_jsons))\n",
    "\n",
    "for row in range(0, len(loaded_jsons)-1, 2):\n",
    "    # print(\"Hi\")\n",
    "    user_content = loaded_jsons[row]\n",
    "    assistant_content = loaded_jsons[(row + 1)]\n",
    "    prompt_row_data = tokenize(generate_prompt(user_content, assistant_content))\n",
    "    train_data.append(prompt_row_data)\n",
    "\n",
    "# train_data.shuffle()\n",
    "\n",
    "\n",
    "# train_data = train_data.shuffle().map(lambda x: tokenize(generate_prompt(x)), remove_columns=[\"modern\" , \"shakespearean\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2931a406-dd66-49c0-9773-e8d8cba52f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf461d-8fb4-4c6a-96e6-8012f3cb64d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='172' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 88/172 19:33 < 19:06, 0.07 it/s, Epoch 1.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.912100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.476900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.350800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.752100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.493000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.373600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.324700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.168800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1.206600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  train_dataset=train_data,\n",
    "  args=TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"mixtral-moe-lora-instruct-shapeskeare\"\n",
    "  ),\n",
    "  data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738fce0-bb2e-435d-85e8-002e80fe386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_prompt(usercontent, assistant_content):\n",
    "#   sys_msg = \"Respond to this in the form of Jules from Pulp Fiction\"\n",
    "#   p = \"<s> [INST]\" + sys_msg +\"\\n\"+ usercontent[\"content\"] + \"[/INST]\" +  assistant_content[\"content\"] + \"</s>\"\n",
    "#   return p \n",
    "\n",
    "# test = {'modern': \"When someone says 'She's thirsty, ain't she?', they're implying she's seeking attention.\", 'shakespearean': 'When one remarks, \"She doth crave attention, doth she not?\", they suggest her desire for notice.'}\n",
    "\n",
    "# usercontent =  {\"role\": \"user\", \"content\": '(yelling to all) Everybody be cool this is a robbery!'}\n",
    "# assistant_content = {'role': 'assistant', 'content': ' Okay now, tell me about the hash bars?'}\n",
    "# print(type(usercontent))\n",
    "# print(usercontent)  # This will show the actual data structure\n",
    "\n",
    "# generate_prompt(usercontent, assistant_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1f2a8-a6fb-4320-b503-2aa3206f8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baea09-e0ce-47b7-acfd-0f954a1333e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def translate_to_shakespearean(text):\n",
    "#     # Generate prompt\n",
    "#     prompt = f\"<s> [INST]Respond to this in the form of Jules from Pulp Fiction\\n{text}[/INST]</s>\"\n",
    "    \n",
    "#     # Tokenize the prompt\n",
    "#     inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\").to(model.device)\n",
    "    \n",
    "#     # Generate output tokens\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, max_length=CUTOFF_LEN)\n",
    "    \n",
    "#     # Decode generated tokens to text\n",
    "#     translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return translation\n",
    "\n",
    "# # Example usage\n",
    "# modern_text = \"hello\"\n",
    "# print(\"Shakespearean translation:\", translate_to_shakespearean(modern_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab701b-ca38-4a3d-ad3e-700cca9260b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_as_Jules(text):\n",
    "    # Generate prompt\n",
    "    prompt = f\"<s> [INST]Respond to this in the form of Jules from Pulp Fiction\\n{text}[/INST]</s>\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=CUTOFF_LEN, padding=\"max_length\").to(model.device)\n",
    "    \n",
    "    # Generate output tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=CUTOFF_LEN)\n",
    "\n",
    "    # Decode generated tokens to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "user_quest = \"Hi, how's it going\"\n",
    "print(\"Jules response:\", respond_as_Jules(user_quest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
